# jieba-wasm 在校对工作中的应用扩展研究

本文档研究 jieba-wasm 现有能力与潜在应用，为 AI Proofreader 扩展的校对功能提供参考。

---

## 一、jieba-wasm 能力概览

| API                           | 用途                 | 当前扩展使用                         |
| ----------------------------- | -------------------- | ------------------------------------ |
| `cut(text, hmm?)`             | 精确模式分词         | ✓ 词级 n-gram、分段命令、异形词检查 |
| `cut_all(text)`               | 全模式（穷举切分）   | 未用                                 |
| `cut_for_search(text, hmm?)`  | 搜索模式（更细粒度） | ✓ cutMode=search 时使用             |
| `tokenize(text, mode, hmm?)`  | 分词 + 起止位置      | ✓ 异形词检查                        |
| `add_word(word, freq?, tag?)` | 添加用户词           | 未用                                 |
| `with_dict(dict)`             | 加载自定义词典       | ✓ 用户词典路径（customDictPath）    |
| `tag(sentence, hmm?)`         | 词性标注             | ✓ 词频统计表                         |

---

## 二、现有应用（已实现）

1. **词级相似度**：句子对齐、引文核对中用词级 n-gram（`getWordNgrams`）替代字级，提升语义匹配
2. **异形词检查**：`scanDocumentWithSegmentation` 按词匹配 variant_to_* 表，减少误报
3. **分词命令**：`segmentFile` / `segmentSelection` 提供全文或选中文本的分词替换，或输出词频统计 CSV 文件（词语、词性、词频，可选简体/繁体）

---

## 三、潜在应用（按实现难度排序）

### 3.1 低难度：自定义词典支持（`with_dict` + `add_word`）

**应用**：专业术语、人名地名、机构名等保持正确切分。

**实现思路**：
- 设置项：`ai-proofread.jieba.customDictPath`（用户词典路径）
- 或：`ai-proofread.jieba.customWords`（词列表，用 `add_word` 逐个添加）
- 在 `getJiebaWasm` 首次加载后，按配置执行 `with_dict` / `add_word`

**适用场景**：医学、法律、古籍等专业书稿，避免「北京大学」被切成「北京」「大学」等。

---

### 3.2 低难度：搜索模式分词（`cut_for_search`）✓ 已实现

**应用**：需要更细粒度切分时（如术语检索、短句匹配）。

**实现**：
- 配置 `ai-proofread.jieba.cutMode`：`default` | `search`
- 在 `getWordNgrams` 中根据模式选择 `cut` 或 `cut_for_search`

**适用场景**：引文核对中，文献句用 search 模式可获得更多子词，便于匹配不完整引文。

---

### 3.3 中难度：「的 / 地 / 得」用法初筛（`tag`）

**应用**：结合词性标注粗筛可能误用。

**用法规则**：
- **的**：定语 + 的 + 名词  
- **地**：状语 + 地 + 动词/形容词  
- **得**：动词/形容词 + 得 + 补语  

**实现思路**：
- 用 `tag(sentence)` 得到 `{ word, tag }[]`
- 遍历 `的/地/得` 时，检查前后词的词性：
  - 「的」：后词应为名词/代词（n/r 等）
  - 「地」：后词应为动词/形容词（v/a 等）
  - 「得」：前词应为动词/形容词（v/a 等）
- 不符合规则时给出提示（需人工确认）

**局限**：jieba 词性不如 Paddle 等深度学习模型准确，可作辅助提示，不宜作为唯一依据。

---

### 3.4 中难度：分句边界优化（`tokenize` + 标点）

**应用**：在 `splitChineseSentences` 中，结合分词边界优化句切分。

**实现思路**：
- 先按标点粗分句，再对每句做 `tokenize`
- 若标点切在词中间（如「北京大学。的」），可结合分词边界微调切分点
- 或：通过分词判断句首是否为完整词，避免「了。他」这类断句

**适用场景**：OCR、PDF 转文本等标点不可靠的场景。

---

### 3.5 中难度：全模式辅助新词发现（`cut_all`）

**应用**：发现可能的新词或未登录词。

**实现思路**：
- 对段落做 `cut(text)` 与 `cut_all(text)`，比较两者结果
- 若 `cut_all` 中有 `cut` 未出现的更长词，可视为候选新词
- 可结合频率或人工确认，加入用户词典

**适用场景**：专业术语、新造词、网络用语等尚未在词典中的词。

---

### 3.6 高难度：术语一致性检查（分词 + 词表）

**应用**：整书术语用法统一。

**实现思路**：
- 首次扫描：提取所有 `cut` 结果中的名词、专有名词（需 `tag`）
- 建立「首选词 ↔ 变体」 mapping（如「人工智能」↔「AI」「人工智慧」）
- 二次扫描：检测变体出现，提示替换为首选词

**依赖**：需维护术语表或从现有词典导入；与当前字词检查的异形词表可复用部分逻辑。

---

### 3.7 高难度：分句后再对齐（cut_for_search + 相似度）

**应用**：引文与文献在分句粒度不一致时的对齐。

**实现思路**：
- 对引文和文献句分别用 `cut_for_search` 得到更细粒度的「子句」
- 用子句级相似度做二次对齐，再映射回原句

**适用场景**：长句被拆成多句，或一方有省略、插入等。

---

## 四、jieba-wasm 不具备的能力

| 能力              | 说明                            |
| ----------------- | ------------------------------- |
| 词典查询          | 无从词典查词频、词性等          |
| TF-IDF / TextRank | jieba-rs 有，但 wasm 绑定未暴露 |
| 删除词            | 无 `del_word`                   |
| 词典导出          | 无法导出当前词典内容            |

如需上述能力，需考虑其他方案（如 Python 子进程、独立服务等）。

---

## 五、建议实施优先级

| 优先级 | 功能                                   | 预期收益             | 工作量 |
| ------ | -------------------------------------- | -------------------- | ------ |
| 1      | 自定义词典（`with_dict` / `add_word`） | 专业稿分词质量提升   | 低     |
| 2      | 「的/地/得」初筛（`tag`）              | 常见语法错误辅助检测 | 中     |
| 3      | 搜索模式可选（`cut_for_search`）       | 引文等场景匹配更细   | 低     |
| 4      | 分句边界优化                           | 标点异常时句子更准确 | 中     |
| 5      | 新词发现（`cut_all`）                  | 支持术语积累         | 中     |
| 6      | 术语一致性检查                         | 整书术语统一         | 高     |

---

## 六、参考

- [jieba-wasm 集成方案](./jieba-wasm-integration-plan.md)
- [jieba 词性标注 ictclas 标记集](https://github.com/fxsjy/jieba)
- 扩展内 `src/jiebaLoader.ts`、`src/similarity.ts`、`src/xh7/documentScanner.ts`
